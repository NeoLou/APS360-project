{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.arange(3*4*4).reshape(3, 4, 4)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = LinearProjector(4, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_patches(input, N):\n",
    "    \"\"\"break tensor input image into N patches\"\"\"\n",
    "    H, W = tuple(input.shape[-2:])\n",
    "    P = int(np.sqrt((H*W)/N))\n",
    "\n",
    "    split1 = torch.split(input, P, -1)\n",
    "    split2 = []\n",
    "    for split in split1:\n",
    "        split2.extend(list(torch.split(split, P, -2)))\n",
    "    return torch.stack(split2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "torch.Size([2, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 3, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_size = 64\n",
    "N = 16\n",
    "P = int(np.sqrt((input_size**2)/N))\n",
    "a = torch.arange(2*(input_size**2)*3).reshape(2, 3, input_size, input_size)\n",
    "split1 = torch.split(a, P, -1)\n",
    "\n",
    "split2 = []\n",
    "for split in split1:\n",
    "    split2.extend(list(torch.split(split, P, -2)))\n",
    "\n",
    "# split2 = (torch.split(splitn, 2, 1) for splitn in split1)\n",
    "# split2\n",
    "print(a.shape)\n",
    "torch.stack(split2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "img = Image.open('0005098900241653513286999.jpg').convert('RGB')\n",
    "img = np.stack([np.array(img)]*10)\n",
    "imgt = torch.from_numpy(img).permute(0, 3, 1, 2)\n",
    "imgt = imgt.type(torch.float64)\n",
    "\n",
    "t = transforms.RandomCrop(464, pad_if_needed=True)\n",
    "imgt = t(imgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 3, 116, 116])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "c = break_into_patches(imgt, 16)\n",
    "print(c.shape)\n",
    "# torchvision.utils.make_grid(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.permute(1, 0, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 40368])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = d.flatten(-3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(16).repeat(10, 1).unsqueeze(2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = torch.cat((a, d), 2)\n",
    "e = e.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 116\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 40369])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = nn.ModuleList([\n",
    "    nn.Linear((P**2)*3 +1, 5) for _ in range(N)\n",
    "])\n",
    "\n",
    "torch.cat([ls[i](e[:, i]) for i in range(N)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 40369])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK LETS TRY IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adil.quettawala/Documents/APS360/Project/APS360-project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_images import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = LinearProjector(16, 5)\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "img = Image.open('0005098900241653513286999.jpg').convert('RGB')\n",
    "img = np.stack([np.array(img)]*10)\n",
    "imgt = torch.from_numpy(img).permute(0, 3, 1, 2)\n",
    "imgt = imgt.type(torch.float64)\n",
    "\n",
    "t = transforms.RandomCrop(464, pad_if_needed=True)\n",
    "imgt = t(imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 464, 464])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = LP.forward(imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = torch.arange(1, 17).repeat(10, 1).unsqueeze(-1)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]],\n",
       "\n",
       "        [[   1.0000,  305.6849,  209.2019, -106.8989,  243.2090,   83.9843],\n",
       "         [   2.0000,   28.4007,   13.6438,  135.1077,  -97.4835, -166.9490],\n",
       "         [   3.0000,  157.5540,   74.9798,  -74.5945,    4.2159,  -64.9919],\n",
       "         [   4.0000,  -52.3303, -163.3389,  270.7256,  228.7937,    1.1391],\n",
       "         [   5.0000,  -58.9649,   30.6662, -136.0254,  137.2398,  -67.5760],\n",
       "         [   6.0000,  127.0598, -139.3164,  -98.9900,  -46.4051,  151.0910],\n",
       "         [   7.0000,  278.4673, -235.2512,  -48.6796,    2.8498,  -88.9378],\n",
       "         [   8.0000, -223.0407,  -79.0655, -167.6257, -130.0516,  114.7287],\n",
       "         [   9.0000,  -93.0002,  -20.9926,   72.6275,  -28.3302,   95.6224],\n",
       "         [  10.0000,   98.8691, -213.1415,   71.3390,  138.2855,  248.8171],\n",
       "         [  11.0000, -217.5755, -174.6820,  -66.4536,   67.2535,  -16.0954],\n",
       "         [  12.0000,   -8.3566,  -40.6561,   71.3283, -160.6714,  116.9346],\n",
       "         [  13.0000, -338.9627, -223.6482,  -61.3419,   73.0948,  156.0408],\n",
       "         [  14.0000,   -0.5143,   36.6002,    7.4515,    3.8517,  232.0534],\n",
       "         [  15.0000,   57.0181,  127.4837,    9.3582,  342.0428,  -21.7801],\n",
       "         [  16.0000, -103.9393,  -73.6766,   51.6788,   32.9543,  -79.8475]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_embed = torch.cat((embed, out), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_embed = (\n",
    "            torch.randn(6)\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "class_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_embed[:, 0] = 0.0\n",
    "ce = class_embed.repeat(10, 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((ce, out), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(class_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = torch.cat(out, 1)\n",
    "out2 = torch.cat(out, 1).reshape(10, 16, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40368])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LP._fc_list[0]._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE MORE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adil.quettawala/Documents/APS360/Project/APS360-project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_images import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = LinearProjector(16, 5)\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "img = Image.open('0005098900241653513286999.jpg').convert('RGB')\n",
    "img = np.stack([np.array(img)]*10)\n",
    "imgt = torch.from_numpy(img).permute(0, 3, 1, 2)\n",
    "imgt = imgt.type(torch.float64)\n",
    "\n",
    "t = transforms.RandomCrop(464, pad_if_needed=True)\n",
    "imgt = t(imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 464, 464])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = LP.forward(imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adil.quettawala/Documents/APS360/Project/APS360-project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adil.quettawala/.virtualenvs/global/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageTransformer(\n",
       "  (criterion): MSELoss()\n",
       "  (_LP): LinearProjector(\n",
       "    (_fc_list): ModuleList(\n",
       "      (0-16): 17 x Linear(in_features=40368, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_LN): LayerNorm((17, 5), eps=1e-05, elementwise_affine=True)\n",
       "  (_TEL): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=5, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=5, bias=True)\n",
       "    (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_TE): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=5, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=5, bias=True)\n",
       "        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((17, 5), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (_fc): Linear(in_features=85, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_images import *\n",
    "\n",
    "IT = ImageTransformer('ViT', 10, 0.01, 200, 16, 5, 1, 1)\n",
    "IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3911],\n",
       "        [0.3177],\n",
       "        [0.2505],\n",
       "        [0.2337],\n",
       "        [0.3913],\n",
       "        [0.3362],\n",
       "        [0.3125],\n",
       "        [0.3564],\n",
       "        [0.2784],\n",
       "        [0.3462]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IT(imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/compute/APS-360\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from common import model\n",
    "from common import evaluate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformer_images import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIT = ImageTransformer('VITb', b, 0.01, 100, 16, 10, 1, 5)\n",
    "train, val = get_data_loaders(VIT.batch_size)\n",
    "# model.train_model(VIT, train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageTransformer(\n",
       "  (criterion): MSELoss()\n",
       "  (_LP): LinearProjector(\n",
       "    (_fc_list): ModuleList(\n",
       "      (0-16): 17 x Linear(in_features=40368, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_LN): LayerNorm((17, 10), eps=1e-05, elementwise_affine=True)\n",
       "  (_TEL): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
       "    (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_TE): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
       "        (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((17, 10), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (_fc): Linear(in_features=170, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIT = ImageTransformer('VITb', 1, 0.01, 100, 16, 10, 1, 5)\n",
    "VIT.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "for t in train:\n",
    "    print(t[0].device)\n",
    "    print(t[1].device)\n",
    "    break\n",
    "\n",
    "for v in val:\n",
    "    print(v[0].device)\n",
    "    print(v[1].device)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
